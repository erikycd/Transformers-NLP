{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b64c3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreated on Wed May 3 11:03:09 2023\\n\\n@author: erikycd\\n\\nTask: Fine-tunning english GPT2 for text generation (chatbot)\\nModel: Pretrained https://huggingface.co/gpt2\\nDataset: Kaggle conversation\\nProcess: \\n    - Activate GPU for Torch\\n    - Loading Model and tokenizer\\n    - Set up special tokens\\n    - Read Data text info\\n    - Text tokenization and pytorch tensor conversion (input_ids, attention_mask)\\n    - DataLoader torch function for batches\\n    - Inference class and function\\n    - Set up hyper-parameters and training stage\\n    - Inference on new data: Conversation\\n    - Saving model and tokenizer\\nSource: Pawan_main.py & Erik_ChatData.py\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Created on Wed May 3 11:03:09 2023\n",
    "\n",
    "@author: erikycd\n",
    "\n",
    "Task: Fine-tunning english GPT2 for text generation (chatbot)\n",
    "Model: Pretrained https://huggingface.co/gpt2\n",
    "Dataset: Kaggle conversation\n",
    "Process: \n",
    "    - Activate GPU for Torch\n",
    "    - Loading Model and tokenizer\n",
    "    - Set up special tokens\n",
    "    - Read Data text info\n",
    "    - Text tokenization and pytorch tensor conversion (input_ids, attention_mask)\n",
    "    - DataLoader torch function for batches\n",
    "    - Inference class and function\n",
    "    - Set up hyper-parameters and training stage\n",
    "    - Inference on new data: Conversation\n",
    "    - Saving model and tokenizer\n",
    "Source: Pawan_main.py & Erik_ChatData.py\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c206d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% IMPORTING LIBRARIES\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "006023b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available for Torch\n",
      "Card name:  NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "#%% SETTING UP DEVICE FOR TORCH\n",
    "\n",
    "gpu_torch = torch.cuda.is_available()\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "print(\"GPU is\", \"available for Torch\" if gpu_torch else \"NOT AVAILABLE\")\n",
    "try:\n",
    "    print('Card name: ', gpu_name)\n",
    "except:\n",
    "    None\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6a3b7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./GPT2_eng/tokenizer_config.json',\n",
       " './GPT2_eng/special_tokens_map.json',\n",
       " './GPT2_eng/vocab.json',\n",
       " './GPT2_eng/merges.txt',\n",
       " './GPT2_eng/added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%% LOADING AND SAVING MODEL/TOKENIZER FROM HUGGINGFACE\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \n",
    "                              \"bos_token\": \"<startofstring>\",\n",
    "                              \"eos_token\": \"<endofstring>\"})\n",
    "tokenizer.add_tokens([\"<bot>:\"])\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "### SAVING IN LOCAL DIRECTORY\n",
    "import os\n",
    "\n",
    "output_dir = './GPT2_eng/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a55b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% LOADING ENGLISH MODEL FROM FILE\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./GPT2_eng/\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \n",
    "                              \"bos_token\": \"<startofstring>\",\n",
    "                              \"eos_token\": \"<endofstring>\"})\n",
    "tokenizer.add_tokens([\"<bot>:\"])\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./GPT2_eng/\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d079d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of tokenizer: 50257\n",
      "Tokenizer info: \n",
      " PreTrainedTokenizer(name_or_path='./GPT2_eng/', vocab_size=50257, model_max_len=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<startofstring>', 'eos_token': '<endofstring>', 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<pad>'}) \n",
      "\n",
      "The padding token is <pad> and the ID for the PAD token is 50257.\n",
      "The begining_of_sentence token is <startofstring> and the ID for the BOS token is 50258.\n",
      "The end_of_sentence token is <endofstring> and the ID for the EOS token is 50259.\n",
      "The bot token is <bot>: and its ID is [50260].\n"
     ]
    }
   ],
   "source": [
    "#%% PRINT SPECIAL TOKENS FOR MODEL\n",
    "\n",
    "print('Length of tokenizer:', tokenizer.vocab_size)\n",
    "print('Tokenizer info: \\n', tokenizer,  '\\n')\n",
    "\n",
    "print(f'The padding token is {tokenizer.pad_token} and the ID for the PAD token is {tokenizer.pad_token_id}.')\n",
    "print(f'The begining_of_sentence token is {tokenizer.bos_token} and the ID for the BOS token is {tokenizer.bos_token_id}.')\n",
    "print(f'The end_of_sentence token is {tokenizer.eos_token} and the ID for the EOS token is {tokenizer.eos_token_id}.')\n",
    "print(f'The bot token is <bot>: and its ID is {tokenizer(\"<bot>:\").input_ids}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "548ee12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 139409 entries, 0 to 139408\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   Unnamed: 0       139409 non-null  int64 \n",
      " 1   question         139409 non-null  object\n",
      " 2   answer           139409 non-null  object\n",
      " 3   question_as_int  139409 non-null  object\n",
      " 4   answer_as_int    139409 non-null  object\n",
      " 5   question_len     139409 non-null  int64 \n",
      " 6   answer_len       139409 non-null  int64 \n",
      "dtypes: int64(3), object(4)\n",
      "memory usage: 7.4+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well, I thought we'd start with pronunciation,...</td>\n",
       "      <td>Not the hacking and gagging and spitting part....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not the hacking and gagging and spitting part....</td>\n",
       "      <td>Okay... then how 'bout we try out some French ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You're asking me out.  That's so cute. What's ...</td>\n",
       "      <td>Forget it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No, no, it's my fault -- we didn't have a prop...</td>\n",
       "      <td>Cameron.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gosh, if only we could find Kat a boyfriend...</td>\n",
       "      <td>Let me see what I can do.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Well, I thought we'd start with pronunciation,...   \n",
       "1  Not the hacking and gagging and spitting part....   \n",
       "2  You're asking me out.  That's so cute. What's ...   \n",
       "3  No, no, it's my fault -- we didn't have a prop...   \n",
       "4     Gosh, if only we could find Kat a boyfriend...   \n",
       "\n",
       "                                              answer  \n",
       "0  Not the hacking and gagging and spitting part....  \n",
       "1  Okay... then how 'bout we try out some French ...  \n",
       "2                                         Forget it.  \n",
       "3                                           Cameron.  \n",
       "4                          Let me see what I can do.  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%% READING DATA TEXT INFO DATASET\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "path = 'G:/Otros ordenadores/Mi PC/Python ML/Language Dataset/kaggle_2'\n",
    "\n",
    "data = pd.read_csv(path + '/dialogs_expanded.csv')\n",
    "questions = list(data['question'])\n",
    "answers = list(data['answer'])\n",
    "\n",
    "print(data.info())\n",
    "\n",
    "data.loc[:,['question', 'answer']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "592e4454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% FUNCTION FOR CLEANING DATA AND CONVERTING (TEXT -> PYTORCH TENSORS)\n",
    "### INPUTS: Data_path, Tokenizer\n",
    "### OUTPUTS: Encoded sentences, attention masks\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "class ChatData_kaggle2(Dataset):\n",
    "    \n",
    "    def __init__(self, path, tokenizer):\n",
    "        \n",
    "        data = pd.read_csv(path)\n",
    "        \n",
    "        questions = list(data['question'])\n",
    "        answers = list(data['answer'])\n",
    "        \n",
    "        clean_questions = []\n",
    "        clean_answers = []\n",
    "\n",
    "        for line in questions:\n",
    "            clean_questions.append(self.clean_text_eng(line))\n",
    "                \n",
    "        for line in answers:\n",
    "            clean_answers.append(self.clean_text_eng(line))\n",
    "            \n",
    "        self.X = []\n",
    "\n",
    "        for ques, ans in zip(clean_questions, clean_answers):\n",
    "            \n",
    "            sequence = \"<startofstring> \" + ques + \" <bot>: \" + ans + \" <endofstring>\"\n",
    "            self.X.append(sequence)\n",
    "        \n",
    "        # self.X = self.X[:50000]\n",
    "        # print(self.X[0])\n",
    "\n",
    "        self.X_encoded = tokenizer(self.X,max_length=40, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        self.input_ids = self.X_encoded['input_ids']\n",
    "        self.attention_mask = self.X_encoded['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.input_ids[idx], self.attention_mask[idx])\n",
    "    \n",
    "    def clean_text_eng(self, text):\n",
    "\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"  \", \" \", text)\n",
    "        text = re.sub(r\"i'm\", \"i am\", text)\n",
    "        text = re.sub(r\"he's\", \"he is\", text)\n",
    "        text = re.sub(r\"she's\", \"she is\", text)\n",
    "        text = re.sub(r\"it's\", \"it is\", text)\n",
    "        text = re.sub(r\"that's\", \"that is\", text)\n",
    "        text = re.sub(r\"what's\", \"that is\", text)\n",
    "        text = re.sub(r\"where's\", \"where is\", text)\n",
    "        text = re.sub(r\"how's\", \"how is\", text)\n",
    "        text = re.sub(r\"let's\", \"let us\", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"won't\", \"will not\", text)\n",
    "        text = re.sub(r\"can't\", \"cannot\", text)\n",
    "        text = re.sub(r\"n't\", \" not\", text)\n",
    "        text = re.sub(r\"n'\", \"ng\", text)\n",
    "        text = re.sub(r\"'bout\", \"about\", text)\n",
    "        text = re.sub(r\"'til\", \"until\", text)\n",
    "        text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "        \n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7673d016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversación:  <startofstring> she okay <bot>: i hope so <endofstring>\n",
      "Conversación encoded:  tensor([50258,  7091,  8788, 50260,    72,  2911,   523, 50259, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257])\n",
      "Máscara de atención:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Conversación decoded:  <startofstring> she okay <bot>: i hope so <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "#%% PYTORCH DATA\n",
    "\n",
    "path_text = \"G:/Otros ordenadores/Mi PC/Python ML/Language Dataset/kaggle_2/dialogs_expanded.csv\"\n",
    "\n",
    "chatData = ChatData_kaggle2(path_text, tokenizer)\n",
    "\n",
    "conversacion_ejemplo = 20\n",
    "\n",
    "sentence = chatData.X[conversacion_ejemplo]\n",
    "sentence_encoded = chatData.input_ids[conversacion_ejemplo]\n",
    "sentence_attn = chatData.attention_mask[conversacion_ejemplo]\n",
    "\n",
    "print('Conversación: ', sentence)\n",
    "print('Conversación encoded: ', sentence_encoded)\n",
    "print('Máscara de atención: ', sentence_attn)\n",
    "\n",
    "sentence_decoded = tokenizer.decode(sentence_encoded)\n",
    "print('Conversación decoded: ', sentence_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6b25060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% PYTORCH DATA LOADER\n",
    "\n",
    "batch_sizes = 64\n",
    "train_dataloader = DataLoader(chatData,  # training instances\n",
    "                              sampler = RandomSampler(chatData), # Pull out batches randomly\n",
    "                              batch_size = batch_sizes # train with this batch size.\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52e925e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% INFERENCE FUNCTION\n",
    "\n",
    "class Response:\n",
    "    \n",
    "    def greedy(self, model, tokenizer, bot_input_ids, attn):\n",
    "        chat_history_ids = model.generate(\n",
    "            bot_input_ids, \n",
    "            attention_mask = attn,\n",
    "            max_length = 30,\n",
    "            pad_token_id = tokenizer.pad_token_id\n",
    "            )\n",
    "        return chat_history_ids\n",
    "    \n",
    "    def beam(self, model, tokenizer, bot_input_ids, attn):\n",
    "        chat_history_ids = model.generate(\n",
    "            bot_input_ids,\n",
    "            attention_mask = attn,\n",
    "            max_length = 30,\n",
    "            num_beams = 3,\n",
    "            early_stopping = True,\n",
    "            pad_token_id = tokenizer.pad_token_id\n",
    "            )\n",
    "        return chat_history_ids\n",
    "    \n",
    "    def sampling(self, model, tokenizer, bot_input_ids, attn):\n",
    "        chat_history_ids = model.generate(\n",
    "            bot_input_ids,\n",
    "            attention_mask = attn,\n",
    "            max_length = 30,\n",
    "            do_sample = True,\n",
    "            top_k = 100,\n",
    "            temperature = 0.75,\n",
    "            pad_token_id = tokenizer.pad_token_id\n",
    "            )\n",
    "        return chat_history_ids\n",
    "    \n",
    "    def nucleus(self, model, tokenizer, bot_input_ids, attn):\n",
    "        chat_history_ids = model.generate(\n",
    "            bot_input_ids,\n",
    "            attention_mask = attn,\n",
    "            max_length = 30,\n",
    "            do_sample = True,\n",
    "            top_p = 0.95,\n",
    "            top_k = 0,\n",
    "            temperature = 0.75,\n",
    "            pad_token_id = tokenizer.pad_token_id\n",
    "            )\n",
    "        return chat_history_ids\n",
    "\n",
    "\n",
    "def inference(text):\n",
    "    \n",
    "    inp = \"<startofstring> \" + text + \" <bot>: \"\n",
    "    inp = tokenizer(inp, return_tensors = \"pt\")\n",
    "    X = inp[\"input_ids\"].to(device)\n",
    "    a = inp[\"attention_mask\"].to(device)\n",
    "    response = Response()\n",
    "    output = response.nucleus(model, tokenizer, X, a)\n",
    "    output = tokenizer.decode(output[0])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37dc84da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 1/5 [03:10<12:40, 190.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> hello, how are you? <bot>: sure it is fine <bot>: i just want to come with me by the second way let me know <endofstring> <pad> <pad>\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▏                                                 | 2/5 [06:17<09:26, 188.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> hello, how are you? <bot>: i do not know where you were all day <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▊                                 | 3/5 [09:26<06:17, 188.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> hello, how are you? <bot>: i am fine <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 4/5 [12:35<03:08, 188.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> hello, how are you? <bot>: a quick check on your pants <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [15:44<00:00, 188.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<startofstring> hello, how are you? <bot>: well i am fine <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#%% SETTING UP PARAMETERS AND OWN TRAINING STAGE\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import tqdm\n",
    "\n",
    "epochs = 5\n",
    "learning_rate = 1e-5\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr = learning_rate,\n",
    "                              )\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "class training:\n",
    "    \n",
    "    def v1():\n",
    "    \n",
    "        for epoch in tqdm.tqdm(range(epochs)):\n",
    "\n",
    "            print('Training...')\n",
    "            model.train()\n",
    "            total_train_loss = 0\n",
    "            \n",
    "            # ========== Training ==========\n",
    "\n",
    "            for ii, att in train_dataloader:\n",
    "\n",
    "                inputs_encoded = ii.to(device)\n",
    "                att_mask = att.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss = model(inputs_encoded, \n",
    "                             attention_mask = att_mask, \n",
    "                             labels = inputs_encoded).loss\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "            # ========== Validation ==========\n",
    "\n",
    "            # torch.save(model.state_dict(), \"model_state.pt\")\n",
    "            print(inference('hello, how are you?'))\n",
    "            \n",
    "    def v2():\n",
    "        \n",
    "        for epoch in tqdm.tqdm(range(epochs)):\n",
    "            \n",
    "            print('Training...')\n",
    "            model.train()\n",
    "            total_train_loss = 0\n",
    "            \n",
    "            # ========== Training ==========\n",
    "\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "                b_input_ids = batch[0].to(device)\n",
    "                b_labels = batch[0].to(device)\n",
    "                b_masks = batch[1].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(b_input_ids,\n",
    "                              attention_mask = b_masks,\n",
    "                              labels = b_labels,\n",
    "                              token_type_ids = None )\n",
    "\n",
    "                outputs.loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                batch_loss = outputs.loss.item()\n",
    "                total_train_loss += batch_loss\n",
    "                \n",
    "            # ========== Validation ==========\n",
    "\n",
    "            # torch.save(model.state_dict(), \"model_state.pt\")\n",
    "            print(inference('hello, how are you?'))\n",
    "        \n",
    "        \n",
    "training.v1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9400df50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: Hello there\n",
      "Tiempo de respuesta del chatbot:  0.340 seg\n",
      "Bot:  <startofstring> Hello there <bot>: please stop <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "User: what happened?\n",
      "Tiempo de respuesta del chatbot:  0.377 seg\n",
      "Bot:  <startofstring> what happened? <bot>: i cannot remember <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "User: how are you?\n",
      "Tiempo de respuesta del chatbot:  0.158 seg\n",
      "Bot:  <startofstring> how are you? <bot>: here we go <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "User: i am glad to read that\n",
      "Tiempo de respuesta del chatbot:  0.340 seg\n",
      "Bot:  <startofstring> i am glad to read that <bot>: if you have not read it yet i am not so sure <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "User: you are weird\n",
      "Tiempo de respuesta del chatbot:  0.323 seg\n",
      "Bot:  <startofstring> you are weird <bot>: i did not know you were in the cab <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "User: i am at home\n",
      "Tiempo de respuesta del chatbot:  0.291 seg\n",
      "Bot:  <startofstring> i am at home <bot>: you can see the window <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "User: yes i do\n",
      "Tiempo de respuesta del chatbot:  0.385 seg\n",
      "Bot:  <startofstring> yes i do <bot>: all right good but you have got to go <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "User: bye\n",
      "Tiempo de respuesta del chatbot:  0.366 seg\n",
      "Bot:  <startofstring> bye <bot>: gimme that <bot>: i would better go <endofstring> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "#%% INFERENCE ON NEW DATA\n",
    "\n",
    "import time\n",
    "\n",
    "exit_commands = ('bye', 'quit')\n",
    "text = ''\n",
    "\n",
    "while text not in exit_commands:\n",
    "    \n",
    "    text = input('\\nUser: ')\n",
    "    inicio = time.time()\n",
    "    output = inference(text)\n",
    "    fin = time.time()\n",
    "    print('Tiempo de respuesta del chatbot: ', \"{:.3f}\".format(fin-inicio), 'seg')\n",
    "    print('Bot: ', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0ba78ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./GPT2_eng_finetune/\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "#%% SAVING MODEL AND TOKENIZER\n",
    "\n",
    "import os\n",
    "\n",
    "output_dir = './GPT2_eng_finetune/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print('Model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc08d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
